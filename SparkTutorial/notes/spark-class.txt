Blocksize :
	Windows : 4kB
	Hadoop(HDFS) : 128MB 
----------------------------	
f1->6KB 	
f2->4KB	

way-1
[f1(4KB)]	[f1(2KB) f2(2K)]	[f2(2k)]

way-2
[ f1(4KB) ]	[f1(2KB)]	[f2(4K)]



=================
Setup Spark project in Eclipse using maven
---------------------------------	
JAR -> Java ARchive ( Libraries )

Dependency :
	Way-1 (normal way): Downoload the respective jar files and use it
	Way-2 (standard way): Use build tools
	
What is build tool?
Build tools are used to download/setup dependencies or libraries
Example : maven, ivy, gradle,etc
We are going to use maven

=======================================
Big Data:
	1. Huge Volume
	2. High Velocity
	3. High Variety 
		- Structured(data in table) 
		- Semi-structured(json,csv) 
		- Unstructured(audio,video)
		
Two challenges :
	1. Storage
	2. Data Processing
	
Single machine


Cluster - group of machines 

Distributed Computing
	
To solve this problem :
Google relaesed two white papers:
	1. GFS (Google File System) - To solve storage problem
	2. GMR (Google   MapReduce) - Processing framework
	
----------------------------
Yahoo
Doug Cutting + Mike Cafarella ;
	- They have implemented both white papers:
	- Hadoop = HDFS + MapReduce
		- HDFS(Hadoop Distributed File System) <---- implementation of GFS
		- MapReduce <---- implementation of GMR
Hadoop was donated to Apache -- Apache Hadoop
-----------------------------
Apache Hive
Apache Kafka
Apache Sqoop
-------------------
Spark -> 
	- Enhancement over MapReduce.
	- It is not replacement of MapReduce
	- Independent of hadoop
	
-------------------
Apache Spark :
	- Apache Spark is an open-source unified analytics engine for large-scale data processing.
	- Java, Scala, Python, R
Spark core	- (RDD) - sparkContext
Spark SQL	- (Dataframe and Dataset) - sparkSession/sqlContext
Spark Streaming - ( DStream, strutured streaming ) - sparkSession/streamingContext
Spark ML
Spark GraphQL

------------------------
Scala
	main() method --> entry point for any Scala program
	
Spark
	sparkContext --> entry point for any Spark application
	1. [Before Spark 2.0] sparkContext -> sqlContext, streamingContext 
	2. [Since  Spark 2.0] sparkSession = (sparkContext + sqlContext + streamingContext)
	
-----------------------
RDD -> Resilient Distributed Dataset
	- The main abstraction Spark 
	- It is a collection of elements partitioned across the nodes of the cluster 
		that can be operated on in parallel.
	- Why Resilient ? - RDDs automatically recover from node failures.
	- Immutable
	- Lazy evaluation

How to create RDD?
	1. Using any Scala collection (List, Set, Map, Seq)
		sc.parallelize(<collection>,<partitions>)
	2. By reading files from HDFS, local FS,etc
		sc.textFile(<paths>) 
			- Used to read single/multiple text files and returns single RDD[String]
		sc.wholeTextFiles(<path>) 
			- Used to read single/multiple text files and returns single RDD[Tuple[String,String]]
			- where first value(._1) is file name and second value(._2) is content of the file
	
	
Two types of operations on RDD :
	1. Transformation (map,flatMap, reduceByKey, filter, repartition, coalesce) -- lazy
		a. Narrow transformation - Transformation which do not involve shuffling (map,filter)
		b. Wide Transformation - Transformation which involve shuffling ( repartition,coalesce,reduceByKey  )
	2. Action (sum,count,collect,foreach, write)

-----------------Partition-1--------------	
1            		1
2 -map(square)--> 	4  -filter(even)-->	4
3            		9                 
-----------------Partition-2--------------            
4            		16                	16
5 -map(square)--> 	25 -filter(even)-->
6            		36                	36
-----------------Partition-3--------------            
7            		49                
8 -map(square)--> 	64 -filter(even)-->	64
9            		81

========================= Repartition vs Coalesce=====

Repartition: 
	- Increase or decrease number of partitions
	- It cause more shuffling
	- Try to distribute data across new partitions equally
	
Coalesce :
	- Decrease number of partitions
	- Minimize shuffling
	- Better performance
	- It does not guarantee that each partitions have equal data
-----------------------------
rd1 = rdd	
p1 : 1,2,3
p2 : 4,5,6
p3 : 7,8,9
p4 : 5,6,8,10,11

rd1.repartition(2)
q1 : 1,2,3,7,8,5,11 
q2 : 4,5,6,9,6,8,10
-------------------
rd1.coalesce(2)
p1 : 1,2,3,7,8,9
p4 : 5,6,8,10,11,4,5,6





========================== Map vs FlatMap===========
map --> one to one
RDD[String] -- --map(x=>x.split(" "))-> RDD[Array[String]]
I was learning Scala	--map(x=>x.split(" "))-> Array("I", "was","learning","Scala")
I have completed Scala	--map(x=>x.split(" "))-> Array("I", "have", "completed", "Scala")
--------
flatMap --> one to many
RDD[String] -- --flatMap(x=>x.split(" "))-> RDD[String]

=================== rd1=rd0.flatMap(x=>x.split(" ")) --> No shuffling
----------partition-1 (node-0)
I 
was 
learning 
Scala
----------partition-2 (node-1)
I 
have 
completed 
Scala
=================== rd2=rd1.map(w=>(w,1)) --> No shuffling
----------partition-1 (node-0)
(I,1) 
(was ,1)
(learning ,1)
(Scala,1)
----------partition-2 (node-1)
(I ,1)
(have ,1)
(completed ,1)
(Scala,1)

=================== rd3=rd2.reduceByKey(_+_)  --> shuffling
----------partition-1 (node-0)
(I,2)
(was,1)
(learning ,1)
----------partition-2 (node-1)
(Scala,2)
(have,1)
(completed,1)


Shuffling : Movement of data from one partition( or node) to another partition(or node)

==================Word Count Example========================
Input :

I was learning Scala
I have completed Scala
I started learning Spark using Scala
I will also complete Spark
------------------------------------
We were learning Scala
We have completed Scala
We started learning Spark using Scala
We will also complete Spark


Output :
I	4
We	4
:::
:::
Scala	6
Spark	4


==================

word, o
sc	1
ss  1
sd	3
ss  2
sd  1

sc 1
ss 3
sd 4

select word, sum(o) from tbl group by word

===============================

Raj, IT, 30000
Ram, IT, 50000
San, Bank, 60000
Tan, Bank, 50000

-----
Total salary = 1.9L

select sum(salary) as total_sal from emp;

val totalSal = sc.textFile("path")
				.map(lx=> lx.split(","))
				.map(ar=> ar(2).toDouble)
				.reduce(_+_)

----------
Salary per department:
IT, 80000
Bank, 1.1L

select dept,sum(salary) as total_sal from emp group by dept;

val rd1=sc.textFile("path")	---> RDD[String]
val rd2=rd1.map(lx=> lx.split(","))	---> RDD[Array[String]]
val rd3 = rd2.map(ar=> (ar(1),ar(2).toDouble))	---> RDD[(String, Double)]
(IT, 30000   )
(IT, 50000   )
(IT, 60000   )
(Bank, 60000 )
(Bank, 50000 )
val deptSal=rd3.reduceByKey(_+_)
val deptSal=rd3.reduceByKey((a,b)=>a+b))

IT -> 30000,50000,60000 -> 1.4L
Bank -> 60000, 50000 -> 1.1L

val deptSal = sc.textFile("path")
				.map(lx=> lx.split(","))
				.map(ar=> (ar(1),ar(2).toDouble))
				.reduceByKey((a,b)=>a+b))

------------------------------
val s = (((4 + 8) + 9) + 2 )
12 + 9 + 2 
21+2
23

===================Spark SQL
Spark core - RDD
Spark SQL - Dataframe, Dataset

RDD -> Dataframe -> Dataset

Dataframe :
	- Distributed collection of data organized into named columns
	- Conceptually, it is equivalent to a table in RDBS or dataframe in R/Python
	- How to create DF?
		- existing RDD
		- structred files
		- tables in any database
