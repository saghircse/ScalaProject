Setup Spark project in Eclipse using maven
---------------------------------	
JAR -> Java ARchive ( Libraries )

Dependency :
	Way-1 (normal way): Downoload the respective jar files and use it
	Way-2 (standard way): Use build tools
	
What is build tool?
Build tools are used to download/setup dependencies or libraries
Example : maven, ivy, gradle,etc
We are going to use maven

=======================================
Big Data:
	1. Huge Volume
	2. High Velocity
	3. High Variety 
		- Structured(data in table) 
		- Semi-structured(json,csv) 
		- Unstructured(audio,video)
		
Two challenges :
	1. Storage
	2. Data Processing
	
Single machine


Cluster - group of machines 

Distributed Computing
	
To solve this problem :
Google relaesed two white papers:
	1. GFS (Google File System) - To solve storage problem
	2. GMR (Google   MapReduce) - Processing framework
	
----------------------------
Yahoo
Doug Cutting + Mike Cafarella ;
	- They have implemented both white papers:
	- Hadoop = HDFS + MapReduce
		- HDFS(Hadoop Distributed File System) <---- implementation of GFS
		- MapReduce <---- implementation of GMR
Hadoop was donated to Apache -- Apache Hadoop
-----------------------------
Apache Hive
Apache Kafka
Apache Sqoop
-------------------
Spark -> 
	- Enhancement over MapReduce.
	- It is not replacement of MapReduce
	- Independent of hadoop
	
-------------------
Apache Spark :
	- Apache Spark is an open-source unified analytics engine for large-scale data processing.
	- Java, Scala, Python, R
Spark core	- (RDD)
Spark SQL	- (Dataframe and Dataset)
Spark Streaming - ( DStream, strutured streaming )
Spark ML
Spark GraphQL

------------------------
Scala
	main() method --> entry point for any Scala program
	
Spark
	sparkContext --> entry point for any Spark application
	1. [Before Spark 2.0] sparkContext -> sqlContext, streamingContext 
	2. [Since  Spark 2.0] sparkSession = (sparkContext + sqlContext + streamingContext)
	
-----------------------
RDD -> Resilient Distributed Dataset
	- The main abstraction Spark 
	- It is a collection of elements partitioned across the nodes of the cluster 
		that can be operated on in parallel.
	- Why Resilient ? - RDDs automatically recover from node failures.
	- Immutable
	- Lazy evaluation

How to create RDD?
	1. Using any Scala collection (List, Set, Map, Seq)
	2. By reading files from HDFS, local FS,etc
	
Two types of operations on RDD :
	1. Transformation (map,flatMap) -- lazy
	2. Action (sum,count,collect,foreach, write)

-----------------Partition-1--------------	
1            		1
2 -map(square)--> 	4  -filter(even)-->	4
3            		9                 
-----------------Partition-2--------------            
4            		16                	16
5 -map(square)--> 	25 -filter(even)-->
6            		36                	36
-----------------Partition-3--------------            
7            		49                
8 -map(square)--> 	64 -filter(even)-->	64
9            		81

=================






















	
